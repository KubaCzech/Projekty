{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining laboratory - Introduction\n",
    "\n",
    "**Author: Kuba Czech, 156035**\n",
    "\n",
    "Welcome to the data mining class. During our meetings, we will be dealing with processing and exploring data with the use of the Python language in the Jupyter Notebook setting. We are also going to use low-code and no-code solutions to the presented problems. Today, we are going to set up our working stations and get familiar with the setup.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Course assignements\n",
    "\n",
    "This course consists of X notebooks, X homeworks and 3 assignments. In order to get a pass mark, you need to complete all homeworks. You can get a maximum of 4 points for each assignment. Once the assignment is announced you have two weeks to complete it. Each week of delay deducts 1 point from the mark you get. The amount of points you gather during the course will indicate your final grade.\n",
    "\n",
    "| Points| Grade |\n",
    "| --- | --- |\n",
    "| 0| 2.0  |\n",
    "| 6 | 3.0 |\n",
    "| 7.5 | 3.5 |\n",
    "| 9 | 4.0 |\n",
    "| 10.5 | 4.5 |\n",
    "| 11.5 | 5.0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is the Jupyter Notebook?\n",
    "\n",
    "It's a computing platform that is very commonly used for code presentation, on-hand code execution, as well as preparing code snippets, which later on might be used in a larger library. In this setting you can easily combine Markdown text and executable Python code. This format is very popular in machine learning, data mining and artificial intelligence field in general. A single file in this setting is very often referred to as just a *Notebook*. The file you are viewing right now is a Notebook. Notebook files are usually named with the extension *.ipynb*, which stems from the original open-source project name *IPython Notebook*. The Notebook uses an interactive kernel,  which allows us to maintain the current execution of the code. During the execution, all variables, defined functions, and classes, etc. are stored in the memory, which gives us flexible access to everything we coded (this is nothing new compared to a standard Python interpreter). The Notebooks are delivered to us in several different settings, here are some:\n",
    "  \n",
    "  - **Advanced, modern IDE, which supports Jupyter Notebooks.** In this setting, the IDE is responsible for setting up the interactive kernel with the use of the Python interpreter. A good example of an IDE, which supports the Jupyter Notebooks is Visual Studio Code. Prior to using this option, we need to set up the Python interpreter on the machine.\n",
    "  \n",
    "      **Pros**:\n",
    "\n",
    "         * full customization\n",
    "         * full access to data on hand\n",
    "         * usually supports version control\n",
    "         * easy setup process\n",
    "      **Cons**:\n",
    "      \n",
    "         * you need to set up an IDE on every machine you work on\n",
    "         * requires installation of Python interpreter on the machine\n",
    "\n",
    "\n",
    "  - **A stand-alone Jupyter Notebook server.** This is the original method of delivering the Notebooks. In order to use this setting, one must download and run the Jupyter Server as a separate process on a machine on hand. The Jupyter Notebook server often comes in bundle with complete Python distributions (e.g. WinPython), in that case, the server executable file is usually within the Python folder. The Jupyter Notebook server allows us to access, view and run the notebooks via the web application accessible through a browser. The server allows us to set up the connection details (e.g. the IP address, port, authentication method, password). If you want to use the server in a public network. you need to be very careful while using this option, as it allows an easy access to the Remote Code Execution, which is a substantial vulnerability. Whoever has the access to the *Notebooks* via the server, essentially has the same privileges, as the user, who started the server. Nothing stops us from using the server on the *localhost*. Running the server in a default setting is as simple as running the command:\n",
    "\n",
    "                 jupyter notebook\n",
    "  \n",
    "      Once the server is running, you have access to files and directories, starting with the directory on which, the server was started. Opening the notebook file, switches the application view, so that you can execute the code and read the markdown.\n",
    "      \n",
    "      **Pros**:\n",
    "\n",
    "         * full customization\n",
    "         * access to data on the server machine\n",
    "         * ability to use it in a network setting with many users and a single server\n",
    "      **Cons**:\n",
    "      \n",
    "         * fairly hard setup process (if you want to use it with several users in a network setting)\n",
    "         * if you do not have a server machine, you can only run it in an offline setting\n",
    "         * no native support for version control\n",
    "         * requires installation of Python interpreter on the machine\n",
    "          \n",
    "\n",
    "  - **External Notebook server paired up with virtual machine.** In this setting, we are using a virtual machine with a temporary python environment as the working space. Although we are not forced to maintain the Notebook server, this option comes with several limitations. We are forced to follow the rules of the virtual machine provider. Usually we not permitted to use such a notebook in order to host data, download torrents, use it as an SSH server, connect to the remote proxy, etc. (nothing really related to Data Mining). Such a notebook does not have direct access to our files, we usually need to upload the data on the virtual machine (or a cloud drive) in order to process the data. Other than that, we can consume the Nootebook files as normal. A good example of this setting is Google Colaboratory.\n",
    "\n",
    "      **Pros**:\n",
    "\n",
    "         * access to notebooks on any machine with no setup\n",
    "         * limited customization\n",
    "         * ability to modify and create new Notebooks on hand on any machine\n",
    "         * no need to install any software on the machine (except for a browser)\n",
    "      **Cons**:\n",
    "      \n",
    "         * no support for version control\n",
    "         * restrictions of use\n",
    "         * requires an account (e.g. Google Account)\n",
    "         * limited access to data on hand\n",
    "         * requires uploading the data to an external server (usually limitted space)\n",
    "         * limited customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course I propose one of the two options - those options are not obligatory, you can use any setup you want:\n",
    " - Visual Studio Code\n",
    " - Google Colaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Visual Studio Code.\n",
    "\n",
    "In the class we will be using the Google Colaboratory service. However, if you want to make your setup at home, or with a personal laptop, you can use the Visual Studio Code setup. Process of setting it up comprised of 2 (pretty obvious) steps:\n",
    " - installing Python interpreter - \n",
    "    - if you are using a Linux machine, it is very likely you already have the Python interpreter installed. If this is not the case, use your default package manager to install python (i.e. `apt install python3` on Debianoids).\n",
    "    - if you are using a Windows machine I suggest using a [WinPython](https://winpython.github.io/) package. It comes with a pre-installed set of libraries.\n",
    "    - you can also use the [default Python installer](https://www.python.org/downloads/).\n",
    " - installing Visual Studio Code - VSC is an multi-platform IDE. You can find it [here](https://code.visualstudio.com/).\n",
    " \n",
    "Once you have everything installed you need to create a space on the computer for this class (we are going to use toy data sets, so you do not need gigabytes of free space). You start by creating a dedicated directory on your hard drive. Download this notebook (.ipynb version, not the html) and paste it into the newly created directory. Then, you open the Visual Studio Code application and from the File menu you choose the Open Directory option. In the file explorer you should be able to see this notebook. Upon the first execution of the code block you will need to choose a Python interpreter, which you have already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google Colaboratory\n",
    "\n",
    "Using Google Colab is much easier. You just need to download this notebook, log in to your Google account on the [Colaboratory website](https://colab.research.google.com/). From the File menu use the \"Send notebook\" option. Choose the downloaded file. That's it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have everything set up, switch from the HTML version of the notebook to the interactive one (either in Colab or in VSC). Starting the next week you will be downloading and opening the notebook at the beginning of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is a Notebook organized?\n",
    "\n",
    "Each Notebook consists of list of cells. There are two types of cells:\n",
    "\n",
    " - **Code cell** - the code cell is filled with the code in the programming language the Notebook is set up for (usually it's Python). You can execute the code and immediately see the result. Everything that *happened* in the execution is available in the next cell you run. Once the code cell is executed, it is annotated with a number, which refers to the order of execution. The first cell you run will be annotated with number [1], second with number [2], etc. The enumeration helps us to keep up with the current status of execution.\n",
    " - **Markdown cell** - the markdown cell allows us to insert a formatted text into the notebook. The text is formatted with use of the [Markdown](https://www.markdownguide.org/) language. The Markdown is a lightweight markup language, which is used to add simple formatting to plaintext documents. It was created in 2004 by John Gruber. It is one of the most popular markup languages. This is the same language you can use for example in the Discord app.\n",
    " \n",
    "\n",
    " Each of the code cells can be executed at any point. In most of the IDEs we are allowed to run all cells at once, restart the interpreter and clear all variables and definitions, add a new cell, and reorder existing the cells. \n",
    "\n",
    "\n",
    " #### Exercise 1. \n",
    " \n",
    " Execute the cells in the following order:\n",
    "   1. Run cell 2\n",
    "   2. Run cell 1\n",
    "   3. Run cell 3\n",
    "   3. Run cell 2\n",
    "   4. Run cell 3\n",
    "   5. Restart the kernel\n",
    "   6. Run cell 1\n",
    "   7. Run cell 2\n",
    "   8. Run cell 3.\n",
    "   9. Run cell 3.\n",
    "\n",
    "Observe the results and make notes. Can we execute the cell 2 immediately, why? How does the annotation change when we run a single cell multiple times? What is the value of the _ expression? You can restart this exercise by restarting the kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a + 2\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + _\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.\n",
    "\n",
    "Create a new markdown cell directly below this one and use the Markdown language to answer the questions asked in Exercise 1. Use the following features:\n",
    "  - Level 4 heading\n",
    "  - Bullet list\n",
    "  - Bold text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is level 4 heading\n",
    "* **bullet list item 1 and bold text**\n",
    "* bullet list item 2\n",
    "* bullet list item 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell magic\n",
    "\n",
    "In order to use a package in your Python script you need to import it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens when the package is not installed on the machine? Well. Probably you need open the terminal, type an apropriate command and download the package. This even is more complicated when you have no direct access to the machine. In this case we can use something called [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html). Ususally the *Code cell* is interpreted as a python script. However, we can add a special decorator to change its behaviour. When we add `%%bash` at the beginning of the cell it is going to be executed as if it was a bash terminal. So, in order to install the numpy package (it should be already installed), you can create a cell similar to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/Kuba/Library/Python/3.9/lib/python/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%%bash` is not the only magical command out there. Sometimes we will compare time of executions of different code variants. In this case we can use the `%%time` or `%%timeit` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62 µs, sys: 65 µs, total: 127 µs\n",
      "Wall time: 88.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = np.zeros((10000,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.92 s, sys: 75.1 ms, total: 2 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = [[0 for _ in range(10000)] for _ in range(10000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 µs ± 3.67 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = np.zeros((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.9 ms ± 185 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = [[0 for _ in range(1000)] for _ in range(1000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of cell magics can be found [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data sets\n",
    "\n",
    "During the course we will be using different data sets in order to get familiar with data mining techiques. This section illustrates several techniques of loading up the data sets.\n",
    "\n",
    "#### Scikit-learn package\n",
    "\n",
    "Among various packages we are going to use the scikit-learn package (sklearn). Today we will get familiar with the toy data sets, which the package provides. The package provides 7 different data sets (including boston data set, which is deprecated), among them:\n",
    "\n",
    "- Iris data set - The famous Iris database, first used by Sir R.A. Fisher.\n",
    "- Digits data set - The data set contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "- Wine data set - The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators.\n",
    "\n",
    "#### Loading the data set\n",
    "\n",
    "The datasets are loaded into a dictionary-like structure, [sklearn.utils.Bunch](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html). We use a set of dedicated *load* functions to load the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_diabetes, load_linnerud, load_wine\n",
    "\n",
    "iris_data_set = load_iris()\n",
    "breast_cancer_data_set = load_breast_cancer()\n",
    "digits_data_set = load_digits()\n",
    "diabetes_data_set = load_diabetes()\n",
    "linnerud_data_set = load_linnerud()\n",
    "wine_data_set = load_wine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain a description of each of the data sets by using the DESCR field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "  Structure and Classification Rule for Recognition in Partially Exposed\n",
      "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "  on Information Theory, May 1972, 431-433.\n",
      "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "  conceptual clustering system finds 3 classes in the data.\n",
      "- Many, many more ...\n",
      "\n",
      "|details-end|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iris_data_set.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data set consists of a list of entries. Each entry is comprised of a set of features. Each feature has a name, which corresponds to its real source. We can obtain the names of features by using the feature_names field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data_set.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in each of the data sets is organized as a numpy array (more on that next week). We can get to it by using the data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(iris_data_set.data))\n",
    "iris_data_set.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry corresponds to a certain class, we can obtain names of the classes with use of the target_names field, and the list of classes corresponding to each entry with the target field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris_data_set.target_names)\n",
    "print(iris_data_set.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1.\n",
    "\n",
    "Write a function, which processes a sci-kit learn Bunch object. The function is expected to prepare a the data set description. The description has the following format:\n",
    "\n",
    "  `Dataset data_set_name.`\n",
    "\n",
    "  `Number of samples: NNN`\n",
    "\n",
    "  `Number of classes: NNN`\n",
    "\n",
    "  `  Number of samples in class target_name1: NNN`\n",
    "\n",
    "  `  Number of samples in class target_name2: NNN`\n",
    "\n",
    "  `  ...`\n",
    "\n",
    "  `Number of features: NNN`\n",
    "\n",
    "  `  Average value of feature feature_name1: NNN`\n",
    "\n",
    "  `  Standard deviation of feature feature_name1: NNN`\n",
    "\n",
    "  `  Average value of feature feature_name2: NNN`\n",
    "\n",
    "  `  Standard deviation of feature feature_name2: NNN`\n",
    "\n",
    "  `  Average value of feature feature_name3: NNN`\n",
    "\n",
    "  `  Standard deviation of feature feature_name3: NNN`\n",
    "  \n",
    "  `  ...`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Iris\n",
      "Number of samples: 150\n",
      "Number of classes: 3\n",
      "number of samples in class setosa: 50\n",
      "number of samples in class versicolor: 50\n",
      "number of samples in class virginica: 50\n",
      "Number of features: 4\n",
      "Average value of sepal length (cm): 5.84\n",
      "Standard deviation of sepal length (cm): 0.83\n",
      "Average value of sepal width (cm): 3.06\n",
      "Standard deviation of sepal width (cm): 0.44\n",
      "Average value of petal length (cm): 3.76\n",
      "Standard deviation of petal length (cm): 1.77\n",
      "Average value of petal width (cm): 1.2\n",
      "Standard deviation of petal width (cm): 0.76\n",
      "\n",
      "\n",
      "Dataset BC\n",
      "Number of samples: 569\n",
      "Number of classes: 2\n",
      "number of samples in class malignant: 212\n",
      "number of samples in class benign: 357\n",
      "Number of features: 30\n",
      "Average value of mean radius: 14.13\n",
      "Standard deviation of mean radius: 3.52\n",
      "Average value of mean texture: 19.29\n",
      "Standard deviation of mean texture: 4.3\n",
      "Average value of mean perimeter: 91.97\n",
      "Standard deviation of mean perimeter: 24.3\n",
      "Average value of mean area: 654.89\n",
      "Standard deviation of mean area: 351.91\n",
      "Average value of mean smoothness: 0.1\n",
      "Standard deviation of mean smoothness: 0.01\n",
      "Average value of mean compactness: 0.1\n",
      "Standard deviation of mean compactness: 0.05\n",
      "Average value of mean concavity: 0.09\n",
      "Standard deviation of mean concavity: 0.08\n",
      "Average value of mean concave points: 0.05\n",
      "Standard deviation of mean concave points: 0.04\n",
      "Average value of mean symmetry: 0.18\n",
      "Standard deviation of mean symmetry: 0.03\n",
      "Average value of mean fractal dimension: 0.06\n",
      "Standard deviation of mean fractal dimension: 0.01\n",
      "Average value of radius error: 0.41\n",
      "Standard deviation of radius error: 0.28\n",
      "Average value of texture error: 1.22\n",
      "Standard deviation of texture error: 0.55\n",
      "Average value of perimeter error: 2.87\n",
      "Standard deviation of perimeter error: 2.02\n",
      "Average value of area error: 40.34\n",
      "Standard deviation of area error: 45.49\n",
      "Average value of smoothness error: 0.01\n",
      "Standard deviation of smoothness error: 0.0\n",
      "Average value of compactness error: 0.03\n",
      "Standard deviation of compactness error: 0.02\n",
      "Average value of concavity error: 0.03\n",
      "Standard deviation of concavity error: 0.03\n",
      "Average value of concave points error: 0.01\n",
      "Standard deviation of concave points error: 0.01\n",
      "Average value of symmetry error: 0.02\n",
      "Standard deviation of symmetry error: 0.01\n",
      "Average value of fractal dimension error: 0.0\n",
      "Standard deviation of fractal dimension error: 0.0\n",
      "Average value of worst radius: 16.27\n",
      "Standard deviation of worst radius: 4.83\n",
      "Average value of worst texture: 25.68\n",
      "Standard deviation of worst texture: 6.15\n",
      "Average value of worst perimeter: 107.26\n",
      "Standard deviation of worst perimeter: 33.6\n",
      "Average value of worst area: 880.58\n",
      "Standard deviation of worst area: 569.36\n",
      "Average value of worst smoothness: 0.13\n",
      "Standard deviation of worst smoothness: 0.02\n",
      "Average value of worst compactness: 0.25\n",
      "Standard deviation of worst compactness: 0.16\n",
      "Average value of worst concavity: 0.27\n",
      "Standard deviation of worst concavity: 0.21\n",
      "Average value of worst concave points: 0.11\n",
      "Standard deviation of worst concave points: 0.07\n",
      "Average value of worst symmetry: 0.29\n",
      "Standard deviation of worst symmetry: 0.06\n",
      "Average value of worst fractal dimension: 0.08\n",
      "Standard deviation of worst fractal dimension: 0.02\n",
      "\n",
      "\n",
      "Dataset Digits\n",
      "Number of samples: 1797\n",
      "Number of classes: 10\n",
      "number of samples in class 0: 178\n",
      "number of samples in class 1: 182\n",
      "number of samples in class 2: 177\n",
      "number of samples in class 3: 183\n",
      "number of samples in class 4: 181\n",
      "number of samples in class 5: 182\n",
      "number of samples in class 6: 181\n",
      "number of samples in class 7: 179\n",
      "number of samples in class 8: 174\n",
      "number of samples in class 9: 180\n",
      "Number of features: 64\n",
      "Average value of pixel_0_0: 0.0\n",
      "Standard deviation of pixel_0_0: 0.0\n",
      "Average value of pixel_0_1: 0.3\n",
      "Standard deviation of pixel_0_1: 0.91\n",
      "Average value of pixel_0_2: 5.2\n",
      "Standard deviation of pixel_0_2: 4.75\n",
      "Average value of pixel_0_3: 11.84\n",
      "Standard deviation of pixel_0_3: 4.25\n",
      "Average value of pixel_0_4: 11.85\n",
      "Standard deviation of pixel_0_4: 4.29\n",
      "Average value of pixel_0_5: 5.78\n",
      "Standard deviation of pixel_0_5: 5.67\n",
      "Average value of pixel_0_6: 1.36\n",
      "Standard deviation of pixel_0_6: 3.33\n",
      "Average value of pixel_0_7: 0.13\n",
      "Standard deviation of pixel_0_7: 1.04\n",
      "Average value of pixel_1_0: 0.01\n",
      "Standard deviation of pixel_1_0: 0.09\n",
      "Average value of pixel_1_1: 1.99\n",
      "Standard deviation of pixel_1_1: 3.2\n",
      "Average value of pixel_1_2: 10.38\n",
      "Standard deviation of pixel_1_2: 5.42\n",
      "Average value of pixel_1_3: 11.98\n",
      "Standard deviation of pixel_1_3: 3.98\n",
      "Average value of pixel_1_4: 10.28\n",
      "Standard deviation of pixel_1_4: 4.78\n",
      "Average value of pixel_1_5: 8.18\n",
      "Standard deviation of pixel_1_5: 6.05\n",
      "Average value of pixel_1_6: 1.85\n",
      "Standard deviation of pixel_1_6: 3.59\n",
      "Average value of pixel_1_7: 0.11\n",
      "Standard deviation of pixel_1_7: 0.83\n",
      "Average value of pixel_2_0: 0.0\n",
      "Standard deviation of pixel_2_0: 0.06\n",
      "Average value of pixel_2_1: 2.6\n",
      "Standard deviation of pixel_2_1: 3.58\n",
      "Average value of pixel_2_2: 9.9\n",
      "Standard deviation of pixel_2_2: 5.69\n",
      "Average value of pixel_2_3: 6.99\n",
      "Standard deviation of pixel_2_3: 5.8\n",
      "Average value of pixel_2_4: 7.1\n",
      "Standard deviation of pixel_2_4: 6.18\n",
      "Average value of pixel_2_5: 7.81\n",
      "Standard deviation of pixel_2_5: 6.2\n",
      "Average value of pixel_2_6: 1.79\n",
      "Standard deviation of pixel_2_6: 3.26\n",
      "Average value of pixel_2_7: 0.05\n",
      "Standard deviation of pixel_2_7: 0.44\n",
      "Average value of pixel_3_0: 0.0\n",
      "Standard deviation of pixel_3_0: 0.03\n",
      "Average value of pixel_3_1: 2.47\n",
      "Standard deviation of pixel_3_1: 3.15\n",
      "Average value of pixel_3_2: 9.09\n",
      "Standard deviation of pixel_3_2: 6.19\n",
      "Average value of pixel_3_3: 8.82\n",
      "Standard deviation of pixel_3_3: 5.88\n",
      "Average value of pixel_3_4: 9.93\n",
      "Standard deviation of pixel_3_4: 6.15\n",
      "Average value of pixel_3_5: 7.55\n",
      "Standard deviation of pixel_3_5: 5.87\n",
      "Average value of pixel_3_6: 2.32\n",
      "Standard deviation of pixel_3_6: 3.69\n",
      "Average value of pixel_3_7: 0.0\n",
      "Standard deviation of pixel_3_7: 0.05\n",
      "Average value of pixel_4_0: 0.0\n",
      "Standard deviation of pixel_4_0: 0.0\n",
      "Average value of pixel_4_1: 2.34\n",
      "Standard deviation of pixel_4_1: 3.48\n",
      "Average value of pixel_4_2: 7.67\n",
      "Standard deviation of pixel_4_2: 6.32\n",
      "Average value of pixel_4_3: 9.07\n",
      "Standard deviation of pixel_4_3: 6.27\n",
      "Average value of pixel_4_4: 10.3\n",
      "Standard deviation of pixel_4_4: 5.93\n",
      "Average value of pixel_4_5: 8.74\n",
      "Standard deviation of pixel_4_5: 5.87\n",
      "Average value of pixel_4_6: 2.91\n",
      "Standard deviation of pixel_4_6: 3.54\n",
      "Average value of pixel_4_7: 0.0\n",
      "Standard deviation of pixel_4_7: 0.0\n",
      "Average value of pixel_5_0: 0.01\n",
      "Standard deviation of pixel_5_0: 0.15\n",
      "Average value of pixel_5_1: 1.58\n",
      "Standard deviation of pixel_5_1: 2.98\n",
      "Average value of pixel_5_2: 6.88\n",
      "Standard deviation of pixel_5_2: 6.54\n",
      "Average value of pixel_5_3: 7.23\n",
      "Standard deviation of pixel_5_3: 6.44\n",
      "Average value of pixel_5_4: 7.67\n",
      "Standard deviation of pixel_5_4: 6.26\n",
      "Average value of pixel_5_5: 8.24\n",
      "Standard deviation of pixel_5_5: 5.7\n",
      "Average value of pixel_5_6: 3.46\n",
      "Standard deviation of pixel_5_6: 4.33\n",
      "Average value of pixel_5_7: 0.03\n",
      "Standard deviation of pixel_5_7: 0.31\n",
      "Average value of pixel_6_0: 0.01\n",
      "Standard deviation of pixel_6_0: 0.2\n",
      "Average value of pixel_6_1: 0.7\n",
      "Standard deviation of pixel_6_1: 1.75\n",
      "Average value of pixel_6_2: 7.51\n",
      "Standard deviation of pixel_6_2: 5.64\n",
      "Average value of pixel_6_3: 9.54\n",
      "Standard deviation of pixel_6_3: 5.23\n",
      "Average value of pixel_6_4: 9.42\n",
      "Standard deviation of pixel_6_4: 5.3\n",
      "Average value of pixel_6_5: 8.76\n",
      "Standard deviation of pixel_6_5: 6.03\n",
      "Average value of pixel_6_6: 3.73\n",
      "Standard deviation of pixel_6_6: 4.92\n",
      "Average value of pixel_6_7: 0.21\n",
      "Standard deviation of pixel_6_7: 0.98\n",
      "Average value of pixel_7_0: 0.0\n",
      "Standard deviation of pixel_7_0: 0.02\n",
      "Average value of pixel_7_1: 0.28\n",
      "Standard deviation of pixel_7_1: 0.93\n",
      "Average value of pixel_7_2: 5.56\n",
      "Standard deviation of pixel_7_2: 5.1\n",
      "Average value of pixel_7_3: 12.09\n",
      "Standard deviation of pixel_7_3: 4.37\n",
      "Average value of pixel_7_4: 11.81\n",
      "Standard deviation of pixel_7_4: 4.93\n",
      "Average value of pixel_7_5: 6.76\n",
      "Standard deviation of pixel_7_5: 5.9\n",
      "Average value of pixel_7_6: 2.07\n",
      "Standard deviation of pixel_7_6: 4.09\n",
      "Average value of pixel_7_7: 0.36\n",
      "Standard deviation of pixel_7_7: 1.86\n",
      "\n",
      "\n",
      "Set Diabetesis NOT a classification set\n",
      "\n",
      "\n",
      "Dataset Linnerud\n",
      "Number of samples: 20\n",
      "Number of classes: 3\n",
      "number of samples in class Weight: 0\n",
      "number of samples in class Waist: 0\n",
      "number of samples in class Pulse: 0\n",
      "Number of features: 3\n",
      "Average value of Chins: 9.45\n",
      "Standard deviation of Chins: 5.29\n",
      "Average value of Situps: 145.55\n",
      "Standard deviation of Situps: 62.57\n",
      "Average value of Jumps: 70.3\n",
      "Standard deviation of Jumps: 51.28\n",
      "\n",
      "\n",
      "Dataset Wine\n",
      "Number of samples: 178\n",
      "Number of classes: 3\n",
      "number of samples in class class_0: 59\n",
      "number of samples in class class_1: 71\n",
      "number of samples in class class_2: 48\n",
      "Number of features: 13\n",
      "Average value of alcohol: 13.0\n",
      "Standard deviation of alcohol: 0.81\n",
      "Average value of malic_acid: 2.34\n",
      "Standard deviation of malic_acid: 1.12\n",
      "Average value of ash: 2.37\n",
      "Standard deviation of ash: 0.27\n",
      "Average value of alcalinity_of_ash: 19.49\n",
      "Standard deviation of alcalinity_of_ash: 3.34\n",
      "Average value of magnesium: 99.74\n",
      "Standard deviation of magnesium: 14.28\n",
      "Average value of total_phenols: 2.3\n",
      "Standard deviation of total_phenols: 0.63\n",
      "Average value of flavanoids: 2.03\n",
      "Standard deviation of flavanoids: 1.0\n",
      "Average value of nonflavanoid_phenols: 0.36\n",
      "Standard deviation of nonflavanoid_phenols: 0.12\n",
      "Average value of proanthocyanins: 1.59\n",
      "Standard deviation of proanthocyanins: 0.57\n",
      "Average value of color_intensity: 5.06\n",
      "Standard deviation of color_intensity: 2.32\n",
      "Average value of hue: 0.96\n",
      "Standard deviation of hue: 0.23\n",
      "Average value of od280/od315_of_diluted_wines: 2.61\n",
      "Standard deviation of od280/od315_of_diluted_wines: 0.71\n",
      "Average value of proline: 746.89\n",
      "Standard deviation of proline: 314.91\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "def prepare_dataset_description(data_set : Bunch, data_set_name):\n",
    "    try:\n",
    "        data_set['target_names']\n",
    "    except KeyError:\n",
    "        return f\"Set {data_set_name}is NOT a classification set\\n\\n\"\n",
    "    descr = \"\"\n",
    "    descr += f\"Dataset {data_set_name}\\n\"\n",
    "    descr += f\"Number of samples: {len(data_set.data)}\\n\"\n",
    "    descr += f\"Number of classes: {len(data_set.target_names)}\\n\"\n",
    "    for i in range(len(data_set.target_names)):\n",
    "        descr += f\"number of samples in class {data_set.target_names[i]}: {np.count_nonzero(data_set.target == i)}\\n\"\n",
    "    descr += f\"Number of features: {len(data_set.feature_names)}\\n\"\n",
    "    for i in range(len(data_set.feature_names)):\n",
    "        l = []\n",
    "        for j in range(len(data_set.data)):\n",
    "            l.append(data_set.data[j][i])\n",
    "        descr += f\"Average value of {data_set.feature_names[i]}: {round(statistics.mean(l), 2)}\\n\"\n",
    "        descr += f\"Standard deviation of {data_set.feature_names[i]}: {round(statistics.stdev(l), 2)}\\n\"\n",
    "    descr += \"\\n\"\n",
    "    return descr\n",
    "\n",
    "print(prepare_dataset_description(iris_data_set, 'Iris'))\n",
    "print(prepare_dataset_description(breast_cancer_data_set, 'BC'))\n",
    "print(prepare_dataset_description(digits_data_set, 'Digits'))\n",
    "print(prepare_dataset_description(diabetes_data_set, 'Diabetes'))\n",
    "print(prepare_dataset_description(linnerud_data_set, 'Linnerud'))\n",
    "print(prepare_dataset_description(wine_data_set, 'Wine'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0cc2e28e15ec3d399ff2fa987eff54814158b78b1ea93d5ce0744d3e4b658fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
