{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mooJ5vGuciH3"
      },
      "source": [
        "# Attention -- implementing attention between a given decoder state and the encoder states\n",
        "\n",
        "Below, there are two matrices, `encoder_states` and `decoder_states` representing the state of the hidden layer after processing each word by the encoder and the static embedding related to a given input of the decoder. A single hidden layer state contains an embedding of length = 3, which is equal to the size of the embedding in the decoder. In the encoder, we have 4 hidden layer states, because we are processing a sequence consisting of 4 tokens.\n",
        "\n",
        "There are 5 tokens in the decoder, which are generated based on the sequence processed by the encoder.\n",
        "\n",
        "The task is to: a) Calculate the similarity of all embeddings from the decoder (queries) to all embeddings of subsequent states of the encoder (keys) (remember that matrices can be transposed. In NumPy, we transpose a matrix using `matrix_name.T`)\n",
        "\n",
        "b) Softmax (imported from scipy) should be performed on the created similarity matrix. Note: remember to apply softmax in the right dimension. All hidden states of the encoder should be softmaxed in the context of a given decoder state. In scipy, the softmax function includes an `axis` argument which may help.\n",
        "\n",
        "c) Combine the attention matrix from step b) and `encoder_states` to generate a matrix containing context vectors for each token from the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vbq0QW2td41r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# scipy.special.softmax(x, axis=None)\n",
        "\n",
        "encoder_states = np.array(\n",
        "    [[1.2, 3.4, 5.6],   # encoder's hidden layer output at the step 1, related to a given input token, e.g., I\n",
        "    [-2.3, 0.2, 7.2],   # encoder's hidden layer output at the step 2, related to a given token, e.g., like\n",
        "    [10.2, 0.2, 0.3],   # encoder's hidden layer output at the step 3, related to a given token, e.g., NLP\n",
        "    [0.4, 0.7, 1.2]]    # encoder's hidden layer output at the step 4, related to a given token, e.g., \".\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "decoder_states = np.array(\n",
        "    [[0.74, 0.23, 0.56],  # decoder's static word embedding at the step 1, related to a given token, e.g., <BOS>\n",
        "    [7.23, 0.12, 0.55],  # decoder's static word embedding at the step 2, related to a given token, e.g., Ja\n",
        "    [9.12, 4.23, 0.44], # decoder's static word embedding at the step 3, related to a given token, e.g., lubię\n",
        "    [4.1, 3.23, 0.5],    # decoder's static word embedding at the step 4, related to a given token, e.g., przetwarzanie\n",
        "    [5.2, 3.1, 8.5]]     # decoder's static word embedding at the step 5, related to a given token, e.g., języka\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a)\n",
            "[[  4.806   2.376   7.762   1.129]\n",
            " [ 12.164 -12.645  73.935   3.636]\n",
            " [ 27.79  -16.962  94.002   7.137]\n",
            " [ 18.702  -5.184  42.616   4.501]\n",
            " [ 64.38   49.86   56.21   14.45 ]]\n",
            "\n",
            "b)\n",
            "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
            " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
            " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
            " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
            " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
            "\n",
            "c)\n",
            "[[ 9.69108631  0.35799187  0.59163688]\n",
            " [10.2         0.2         0.3       ]\n",
            " [10.2         0.2         0.3       ]\n",
            " [10.2         0.2         0.3       ]\n",
            " [ 1.20254471  3.39909302  5.59850122]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# a)\n",
        "# Similarity is the dot product between decoder token i and decoder token j\n",
        "print(\"a)\")\n",
        "sim = (np.matmul(decoder_states, encoder_states.T)) # Shape (5, 4) == (number of encoder states, number of decoder states)\n",
        "print(sim, end = '\\n\\n')\n",
        "\n",
        "# b)\n",
        "print(\"b)\")\n",
        "sim_soft = softmax(sim, axis = 1) # Shape (5, 4)\n",
        "print(sim_soft, end = '\\n\\n')\n",
        "\n",
        "# c)\n",
        "print(\"c)\")\n",
        "context_vector = np.matmul(sim_soft, encoder_states)\n",
        "print(context_vector, end = '\\n\\n') # Shape (5, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bfd5X4fAJq"
      },
      "source": [
        "Expected outputs:\n",
        "\n",
        "a) [[ 4.806 2.376 7.762 1.129] [ 12.164 -12.645 73.935 3.636] [ 27.79 -16.962 94.002 7.137] [ 18.702 -5.184 42.616 4.501] [ 64.38 49.86 56.21 14.45 ]]\n",
        "\n",
        "b) [[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03] [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31] [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38] [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17] [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
        "\n",
        "c) [[ 9.69108631 0.35799187 0.59163688] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [ 1.20254471 3.39909302 5.59850122]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sPUDVw7fKHJ"
      },
      "source": [
        "# Transformer\n",
        "## Using transformer-based T5 model to solve various NLP tasks in a sequence-to-sequence manner\n",
        "\n",
        "Today we're going to learn a new library -- the HuggingFace **transformers** library (https://huggingface.co/docs/transformers/index) and use it to solve several non-obvious NLP-related problems using the **T5** model\n",
        "\n",
        "\n",
        "HuggingFace transformers is one of the most popular libraries that provide us with a high-level API for using neural networks to solve tasks related to natural language processing, audio processing, computer vision, or even multimodal scenarios in which we have to utilize multiple modalities at once (e.g., answering questions about pictures, information extraction from invoices).\n",
        "\n",
        "First, let's install the dependencies, the `transformers` library itself and the `sentencepiece` module, which helps us tokenize documents and transform tokens into one-hot encodings (we will discuss the idea of sentencepiece later in detail).\n",
        "\n",
        "**Warning**: if you notice some weird exceptions like `cannot call from_pretrained on a None object` somewhere in your code, restart the environment using: Runtime -> restart. Then run the cells with code (without re-installing the libraries) one more time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmRzPimhfRja"
      },
      "outputs": [],
      "source": [
        "!pip install transformers   # install HuggingFace transformers library\n",
        "!pip install sentencepiece  # install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAg0LOEgGj4"
      },
      "source": [
        "The API provided by the `transformer` library is a high-level one. We can download a given model and generate an output using 4 lines of code!\n",
        "\n",
        "Read the docs on the T5 model provided here: https://huggingface.co/docs/transformers/model_doc/t5\n",
        "\n",
        "In the `inference` section, you can find a description showing how we can download a pretrained model, and use it to solve a given task. Simply use the code provided to translate some sentence from English into German!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lYOfsrkeJ6GF"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "829af775db454b5ba867f58aa6106e39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01fb5d09845844da8e4e726eb878ab35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80ab03bfe0da46ca95ea4d104be90d16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b09be847594b437a98b6e125ff319c90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11c4678a22464774957e73a6e1783889",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "302c0e62bbfb4f949e415a5a3e18ff51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "German translation: Hallo, ich liebe die natürliche Sprache Verarbeitung und ich hasse Calculus\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "text = \"translate English to German: Hello, I love Natural Language Processing and I hate Calculus\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "translation_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"], \n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "german_translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "print(\"German translation:\", german_translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z322IklnhQnO"
      },
      "source": [
        "## Various tasks\n",
        "\n",
        "Experiment with some other inputs, e.g., those provided in Figure 1 presented in the paper introducing the T5 model or even a wider list of use cases from  Appendix D provided with the paper. You can find the paper here: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "Note: there are some abbreviations used among the inputs provided, some of them are:\n",
        "-  `stsb`: it stands for the semantic textual similarity benchmark. Given two sentences, we can calculate their semantic similarities, which can help us determine whether one sentence is a paraphrase of the other one.\n",
        "-  `cola`: it stands for the Corpus of Linguistic Acceptability and helps us determine whether a given sentence is grammatical or ungrammatical.\n",
        "\n",
        "If you look at Appendix D, there are more abbreviations, these are related to the names of tasks presented in the GLUE benchmark (available here: https://gluebenchmark.com/tasks) and the SUPERGLUE benchmark (available here: https://super.gluebenchmark.com/tasks). The idea of GLUE and SUPERGLUE is to collect a set of challenging tasks that may be used to evaluate the systems requiring natural language understanding. \n",
        "\n",
        "**Paste some 3 examples of tasks and the inputs you processed in the cell below** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OfxmQwV4lIXw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for task: stsb, model: PavanNeerudu/t5-base-finetuned-stsb\n",
            "Loading tokenizer for task: cola, model: PavanNeerudu/t5-base-finetuned-cola\n",
            "Loading tokenizer for task: summarize, model: t5-large\n",
            "Loading tokenizer for task: translate, model: t5-large\n",
            "Task: stsb sentence1: A man is playing guitar. sentence2: A person is playing an instrument\n",
            "output: 1.4\n",
            "Task: cola: The book on the table is.\n",
            "output: unacceptable\n",
            "Task: cola: She go to store yesterday buyed many apples and he don’t knowed.\n",
            "output: unacceptable\n",
            "Task: summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\n",
            "output: pope died at 88, a man who promoted social and economic justice . he was 88 .\n",
            "Task: translate English to French: I like machine learning\n",
            "output: J'aime l'apprentissage en machine\n",
            "Task: translate English to German: I love you\n",
            "output: Ich liebe dich\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    \"stsb sentence1: A man is playing guitar. sentence2: A person is playing an instrument\",\n",
        "    \"cola: The book on the table is.\",\n",
        "    \"cola: She go to store yesterday buyed many apples and he don’t knowed.\",\n",
        "    \"summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\",\n",
        "    \"translate English to French: I like machine learning\",\n",
        "    \"translate English to German: I love you\"\n",
        "]\n",
        "\n",
        "models = {\n",
        "    \"stsb\": {\n",
        "        \"model_name\": \"PavanNeerudu/t5-base-finetuned-stsb\",\n",
        "        \"tokenizer\": None,\n",
        "        \"model\": None\n",
        "    },\n",
        "    \"cola\": {\n",
        "        \"model_name\": \"PavanNeerudu/t5-base-finetuned-cola\",\n",
        "        \"tokenizer\": None,\n",
        "        \"model\": None\n",
        "    },\n",
        "    \"summarize\": {\n",
        "        \"model_name\": \"t5-large\",\n",
        "        \"tokenizer\": None,\n",
        "        \"model\": None\n",
        "    },\n",
        "    \"translate\": {\n",
        "        \"model_name\": \"t5-large\",\n",
        "        \"tokenizer\": None,\n",
        "        \"model\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "for task in models:\n",
        "    print(f\"Loading tokenizer for task: {task}, model: {models[task]['model_name']}\")\n",
        "    models[task][\"tokenizer\"] = T5Tokenizer.from_pretrained(models[task][\"model_name\"])\n",
        "    models[task][\"model\"] = T5ForConditionalGeneration.from_pretrained(models[task][\"model_name\"])\n",
        "\n",
        "for text in texts:\n",
        "    if text.startswith('cola'):\n",
        "        task = \"cola\"\n",
        "    elif text.startswith('stsb'):\n",
        "        task = \"stsb\"\n",
        "    elif text.startswith('summarize'):\n",
        "        task = \"summarize\"\n",
        "    elif text.startswith('translate'):\n",
        "        task = \"translate\"\n",
        "\n",
        "    tokenizer = models[task][\"tokenizer\"]\n",
        "    model = models[task][\"model\"]\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    translation_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], \n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=50\n",
        "    )\n",
        "    \n",
        "    decoded_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Task: {text}\\noutput: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQvUkC-BlW3q"
      },
      "source": [
        "## Various model types\n",
        "\n",
        "There are several T5 models available, which differ in size (and quality). The bigger the model is, the better output it should generate. Experiment with some models from the following set: \n",
        "- t5-small\n",
        "- t5-base\n",
        "- t5-large\n",
        "- t5-3b\n",
        "- t5-11b\n",
        "\n",
        "and check whether you can observe any difference in the quality of outputs.\n",
        "\n",
        "Also, compare the size of the models, you can use the `model.num_parameters()` function to obtain the parameter number related to each model. For each model you are able to load, provide the size in the cell below (if you can't load a given model because it is too big, no worries, just type 'too big to load')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "1MTMhyyeR3ZJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: t5-small, Number of params: 60506624\n",
            "Task: summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\n",
            "output: he sought to refocus the Catholic Church to promote social and economic justice. he died at 88.\n",
            "Task: translate English to French: I like machine learning\n",
            "output: Je veux apprendre à la machine\n",
            "Task: translate English to German: I love you\n",
            "output: Ich liebe Sie\n",
            "Model: t5-base, Number of params: 222903552\n",
            "Task: summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\n",
            "output: he sought to refocus the Catholic Church to promote social and economic justice . he was 88.\n",
            "Task: translate English to French: I like machine learning\n",
            "output: J'aime l'apprentissage par machine\n",
            "Task: translate English to German: I love you\n",
            "output: Ich liebe Sie\n",
            "Model: t5-large, Number of params: 737668096\n",
            "Task: summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\n",
            "output: pope died at 88, a man who promoted social and economic justice . he was 88 .\n",
            "Task: translate English to French: I like machine learning\n",
            "output: J'aime l'apprentissage en machine\n",
            "Task: translate English to German: I love you\n",
            "output: Ich liebe dich\n"
          ]
        }
      ],
      "source": [
        "# t5-small params number: 60.5 M\n",
        "# t5-base params number: 223 M\n",
        "# t5-large params number: 738 M\n",
        "# t5-3b params number: too big to load\n",
        "# t5-11b params number: too big to load\n",
        "\n",
        "models = [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]\n",
        "models = [\"t5-small\", \"t5-base\", \"t5-large\"]\n",
        "for model_name in models:\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    print(f\"Model: {model_name}, Number of params: {model.num_parameters()}\")\n",
        "\n",
        "    for text in texts:\n",
        "        if text.startswith(\"cola\") or text.startswith(\"stsb\"):\n",
        "            continue\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "        translation_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"], \n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=50\n",
        "        )\n",
        "\n",
        "        decoded_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Task: {text}\\noutput: {decoded_text}\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDud66l6msUI"
      },
      "source": [
        "## Language-specific T5s (OPTIONAL ASSIGNMENT -- you are not required to provide code here)\n",
        "\n",
        "There are even some alternatives to the original T5 models. As the T5 model was trained on English, there are some models available that are specific to other languages, e.g., Polish (for example plT5 proposed by Allegro - https://huggingface.co/allegro/plt5-small). The Polish model was trained to solve a set of tasks collected in the KLEJ benchmark, which represents the Polish analogy to the GLUE benchmark: https://klejbenchmark.com.\n",
        "\n",
        "You can find more details on plT5 in the research paper: https://arxiv.org/pdf/2205.08808.pdf. Table 2 presents some examples of prompts that can be used to solve some of the tasks listed in KLEJ.\n",
        "\n",
        "You can search for an alternative to the original T5, for example, the one related to your language, and experiment with it (**this task is not mandatory**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJZZUkduoEhZ"
      },
      "outputs": [],
      "source": [
        "# (OPTIONAL): If you want, experiment with some alternative models (like language-related, e.g., plT5 related to Polish)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1hsTnxboIe8"
      },
      "source": [
        "## Flan-T5\n",
        "\n",
        "At the end of 2022, an evolution of T5 was proposed called Flan-T5. This model is also provided by the HuggingFace transformer library. Please visit this website: https://huggingface.co/docs/transformers/model_doc/flan-t5 to see how you can use this model (simply change the name of the model to download!). \n",
        "\n",
        "Flan-T5 is much more powerful than T5. You can take a look at Appendix D included in the paper describing Flan T5 to familiarize yourself with some input formats (prompts) and the generated values. The paper is here: https://arxiv.org/pdf/1910.10683.pdf. You should focus on `processed input` fields as they are the representations that the model consumes. Experiment with some selected tasks and see if you can obtain the same results! In the code below, paste some code loading the Flan-T5 model and using it to solve some selected tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ppbmGGqVqERf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task: summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\n",
            "output: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice, has died.\n",
            "\n",
            "Task: translate English to French: I like machine learning\n",
            "output: Je l'aime l'apprentissage par l'ordinateur\n",
            "\n",
            "Task: translate English to German: I love you\n",
            "output: Ich liebe Sie\n",
            "\n",
            "Task: translate German to English: Ich liebe Schokolade.\n",
            "output: I love chocolate.\n",
            "\n",
            "Task: complete: If it rains tomorrow, we will...\n",
            "output: go to the beach\n",
            "\n",
            "Task: write a short story about a dragon who wants to be a chef.\n",
            "output: The dragon is a savage beast that wants to eat all the food in the world. He is a savage beast that wants to eat all the food in the world. He is a savage beast that wants to eat all the food in the world. He is a savage beast that wants to eat all the food in the world. He is a savage beast that wants to eat\n",
            "\n",
            "Task: what is the next number in the sequence: 2, 4, 8, 16, ...?\n",
            "output: 32\n",
            "\n",
            "Task: Paraphrase: The cat jumped over the wall.\n",
            "output: The cat was very fast.\n",
            "\n",
            "Task: Rewrite this in formal language: Gotta go now, see ya!\n",
            "output: I'm going now, see you!\n",
            "\n",
            "Task: Rewrite this in formal language: I don't give a shit\n",
            "output: I don't care.\n",
            "\n",
            "Task: Question: Who wrote Sherlock Holmes? Answer:\n",
            "output: d w thomas\n",
            "\n",
            "Task: Explain quantum physics like I'm 5 years old.\n",
            "output: Quantum physics is the study of the properties of matter.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tasks = [\n",
        "    \"summarize: Pope Francis, who sought to refocus the Catholic Church to promote social and economic justice rather than traditional moral teachings, has died. He was 88.\",\n",
        "    \"translate English to French: I like machine learning\",\n",
        "    \"translate English to German: I love you\",\n",
        "    \"translate German to English: Ich liebe Schokolade.\",\n",
        "    \"complete: If it rains tomorrow, we will...\",\n",
        "    \"write a short story about a dragon who wants to be a chef.\",\n",
        "    \"what is the next number in the sequence: 2, 4, 8, 16, ...?\",\n",
        "    \"Paraphrase: The cat jumped over the wall.\",\n",
        "    \"Rewrite this in formal language: Gotta go now, see ya!\",\n",
        "    \"Rewrite this in formal language: I don't give a shit\",\n",
        "    \"Question: Who wrote Sherlock Holmes? Answer:\",\n",
        "    \"Explain quantum physics like I'm 5 years old.\"\n",
        "    ]\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "for text in tasks:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    translation_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], \n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=100\n",
        "    )\n",
        "\n",
        "    decoded_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Task: {text}\\noutput: {decoded_text}\\n\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwyRmBx6q0iT"
      },
      "source": [
        "## (OPTIONAL) Fine-tuning\n",
        "\n",
        "You can even fine-tune the T5/Flan-T5 model to solve a task you want. You may load an existing T5/Flan-T5 model, which is already trained to solve some tasks, and use the power of transfer learning to learn it to solve some different tasks. This is much better than training a network from scratch and should require fewer training examples. \n",
        "\n",
        "The fine-tuning phase is quite complex. However, you can find the step-by-step description here: https://www.philschmid.de/fine-tune-flan-t5\n",
        "\n",
        "You can try to fine-tune some selected model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frt5p4E_rjGd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
